{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%reload_ext autoreload\n",
    "%autoreload 2\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import torch"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Name generation using bigrams and statistics\n",
    "Below is a code that reads the input file with names and transforms it into a histogram of bigrams. Bigram is a pair of letters that come one after the other.\n",
    "We calculate the probability of each bigram (i.e. if there's a letter \"a\", what are the chances for letters \"b\", \"c\", \"d\" coming next), and we \"roll the dice\", until we predict an end of sequence."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data set contains 32034 words\n"
     ]
    }
   ],
   "source": [
    "# Read the data set\n",
    "words = open('./names.txt', 'r').read().splitlines()\n",
    "print(f\"Data set contains {len(words)} words\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Storing the information in the 2D array\n",
    "# The 2D array rows represent a first letter in a bigram, and the column represents a second letter in a bigram.\n",
    "# A value in (x,y) represents number of occurences of such bigram in the dataset.\n",
    "\n",
    "bigrams = torch.zeros([27, 27], dtype=torch.int32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define char to int and int to char mappings\n",
    "all_chars = sorted(list(set(''.join(words))))\n",
    "stoi = {s: i+1 for i, s in enumerate(all_chars)}\n",
    "stoi['.'] = 0\n",
    "itos = {i: s for s, i in stoi.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "ename": "Exception",
     "evalue": "no word",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mException\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[9], line 6\u001b[0m\n\u001b[0;32m      4\u001b[0m chars \u001b[39m=\u001b[39m [\u001b[39m'\u001b[39m\u001b[39m.\u001b[39m\u001b[39m'\u001b[39m] \u001b[39m+\u001b[39m \u001b[39mlist\u001b[39m(word) \u001b[39m+\u001b[39m [\u001b[39m'\u001b[39m\u001b[39m.\u001b[39m\u001b[39m'\u001b[39m]\n\u001b[0;32m      5\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mlen\u001b[39m(chars) \u001b[39m==\u001b[39m \u001b[39m2\u001b[39m: \n\u001b[1;32m----> 6\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mException\u001b[39;00m(\u001b[39m\"\u001b[39m\u001b[39mno word\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[0;32m      7\u001b[0m \u001b[39mfor\u001b[39;00m ch1, ch2 \u001b[39min\u001b[39;00m \u001b[39mzip\u001b[39m(chars, chars[\u001b[39m1\u001b[39m:]):\n\u001b[0;32m      8\u001b[0m     bigrams[stoi[ch1], stoi[ch2]] \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m \u001b[39m1\u001b[39m\n",
      "\u001b[1;31mException\u001b[0m: no word"
     ]
    }
   ],
   "source": [
    "# Count bigram (2 letter pairs) occurences in the dataset. \n",
    "# At first, <S> was used as a special start character and <E> as special end character, but Andrej changed both to just '.' for efficiency.\n",
    "for word in words:\n",
    "    chars = ['.'] + list(word) + ['.']\n",
    "    if len(chars) == 2: \n",
    "        raise Exception(\"no word\")\n",
    "    for ch1, ch2 in zip(chars, chars[1:]):\n",
    "        bigrams[stoi[ch1], stoi[ch2]] += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(14,14))\n",
    "plt.imshow(bigrams, cmap='Blues')\n",
    "for i in range(len(bigrams)):\n",
    "    for j in range(len(bigrams[0])):\n",
    "        str = itos[i] + itos[j]\n",
    "        plt.text(j, i, str, ha=\"center\", va=\"bottom\", color=\"grey\")\n",
    "        plt.text(j, i, bigrams[i, j].item(), ha=\"center\", va=\"top\", color=\"grey\")\n",
    "plt.axis('off')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Playground with Pytorch generators and dividing tensors with scalar value\n",
    "g = torch.Generator().manual_seed(2147483647)\n",
    "p = torch.rand(3, generator=g)\n",
    "p = p / p.sum()\n",
    "p"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Torch.multinomial\n",
    "# You give me probabilities, I give you integres which are sampled according to the probability distribution.\n",
    "# So if I input a tensor to this function like this:\n",
    "# probabilities = torch.tensor([0.1, 0.2, 0.3, 0.4])\n",
    "# ... and I call the method like this:\n",
    "# samples = probabilities.multinomial(num_samples=5, replacement=True)\n",
    "# ... I can get a following result:\n",
    "# tensor([2, 3, 1, 3, 0])\n",
    "# It means \"I have conducted 5 experiments and randomly generated this sequence of numbers according to your prob distribution. I have drawn value\n",
    "# from index 2 first, then value from index 3, then 1, etc.\"\n",
    "torch.multinomial(p, num_samples=20, replacement=True, generator=g)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Take first row of bigrams (start/end special character)\n",
    "p = bigrams[0].float()\n",
    "# Normalize it - each cell will now have a probablity of occurence and they will all sum to one\n",
    "p = p / p.sum()\n",
    "g = torch.Generator().manual_seed(2147483647)\n",
    "# Generate the next character according to p distribution\n",
    "idx = torch.multinomial(p, num_samples=1, replacement=True, generator=g).item()\n",
    "idx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model smoothing - explained below\n",
    "P = (bigrams+1).float()\n",
    "# Normalize the bigram array with probability distributions.\n",
    "# \"1\" means collapse the 1-st dimension (colums)\n",
    "# TODO write here about the rules of broadcasting - how do you align dimensions and validate that the division works, etc.\n",
    "P /= P.sum(1, keepdim=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Token generation logic\n",
    "row = 0 # Start at index 0 - start token\n",
    "word = ''\n",
    "while True:\n",
    "    selected = torch.multinomial(P[row], num_samples=1, replacement=True, generator=g).item()\n",
    "    print(f'selected {itos[selected]} with prob {P[row][selected]}')\n",
    "    row = selected\n",
    "    if row == 0: break # 'End' character\n",
    "    word += itos[row]\n",
    "word"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Log likelihood\n",
    "One technique to check the quality of the model is to run it against the training data.\n",
    "We want to look at the probability that the code assigns to each of the bigrams.\n",
    "We can compare it against the evenly distributed probablility (if everything is equally likely), as if language was just random. If it's higher than that, that's a good thing, it means that our algorithm learned something from the data.\n",
    "The problem to devise such metric is called \"maximum likelihood estimation\".\n",
    "\n",
    "Likelihood is calculated as a product of the probabilities. But becomes a very small number, so it's impracical. It's better to work with log likelihood function instead. \n",
    "\n",
    "### Smoothing\n",
    "Trick to add fake counts of data. This is to avoid infinite loss (so inifite negative log likelihood) because log(0) = -inf\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function calculating negative log likelihood of the input data. This is a commonly used loss function for classifiers.\n",
    "# Goal: We want to maximize the likelihood of the data w.r.t. to model parameters.\n",
    "# -> This is equivalent to maximizing the log likelihood, as log is monotonic.\n",
    "# -> Equivalent to minimizing the negative log likelihood.\n",
    "# -> Equivalent to minimizing the negative average log likelihood.\n",
    "def calculate_negative_log_likelihood(input, verbose=False):\n",
    "    loglikelihood = 0.0\n",
    "    n = 0\n",
    "    for w in input:\n",
    "        chars = ['.'] + list(w) + ['.']\n",
    "        for ch1, ch2 in zip(chars, chars[1:]): \n",
    "            prob = P[stoi[ch1], stoi[ch2]]\n",
    "            logprob = torch.log(prob)\n",
    "            # log(a*b*c) = log(a) + log(b) + log(c)\n",
    "            loglikelihood += logprob\n",
    "            n += 1\n",
    "            if verbose: print(f'{ch1}{ch2}, prob: {prob:.4f}, logprob: {logprob:.4f}')\n",
    "    # Negate - we want the cost to be higher as the value of log likelihood becomes more negative.\n",
    "    nll = -loglikelihood \n",
    "    return nll, n\n",
    "\n",
    "nll, n = calculate_negative_log_likelihood(words)\n",
    "print(f'average loglikelihood: {nll/n}')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bigrams using neural nets\n",
    "The following is re-doing the same exercise but this time using neural networks, cost function and gradient descent optimization algorithm. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Re-creating the training set, this time splitting into xs and ys instead of the occurences 2D array.\n",
    "# Creating the training set below.\n",
    "xs, ys = [], []\n",
    "\n",
    "for word in words:\n",
    "    chars = ['.'] + list(word) + ['.']\n",
    "    for ch1, ch2 in zip(chars, chars[1:]):\n",
    "        xs.append(stoi[ch1])\n",
    "        ys.append(stoi[ch2])\n",
    "\n",
    "# torch.tensor vs torch.Tensor - the first one infers the dtype automatically, whereas the second one uses float32.\n",
    "# training set: x[i] input should have y[i] output\n",
    "xs = torch.tensor(xs)\n",
    "ys = torch.tensor(ys)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## One-hot encoding \n",
    "The process of one-hot encoding involves creating a binary vector for each unique category in the dataset, with the size of the vector equal to the number of unique categories. Each vector has only one element set to 1, representing the presence of the corresponding category, while all other elements are set to 0.\n",
    "\n",
    "For us, this means 1x27 tensor, with a \"1\" at stoi[character]-th position, and \"0\"s elsewhere."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn.functional as F\n",
    "\n",
    "xenc = F.one_hot(xs, num_classes=27).float() # by default it's torch.int64\n",
    "print(f'one-hot X shape: {xenc.shape}, datatype: {xenc.dtype}')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Modelling neural network and forward() as matrices\n",
    "We will model inputs as Nx27 matrix, and neural network layer as 27xM matrix.\n",
    "In the input matrix, each row is a single example from the training set, encoded in the one-hot vector.\n",
    "In the NN matrix, each **column** is a single neuron. Each row corresponds to weights for given input i_n\n",
    "So, if we have 10 examples in the training set, the input is a 5x27 matrix.\n",
    "If we have a 'network' with just one neuron, NN matrix will be 27x1.\n",
    "The resulting matrix multiplication output will perform dot product of all the weights and inputs in the neuron, and will be 5x1 - the output per each training set example. Then we will only need to add neuron bias and non-linearity like tanh and voila, done."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fills the tensor with normal distribution - i.e. initialize the weights to random according to it.\n",
    "W = torch.randn([27, 1]) # 27 weights for a single neuron\n",
    "xenc[:5] @ W # 5x27 @ 27x1 = 5x1\n",
    "\n",
    "W = torch.randn([27, 27]) # 27 neurons, each connected to 27 inputs.\n",
    "forward = (xenc[:5] @ W)\n",
    "print(forward.shape)\n",
    "print(forward[3, 13]) # What was the output of 13-th neuron on the 3-rd input?"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Softmax\n",
    "Popular NN layer, that takes the output of the NN layer - some numbers aka logits, and transforms them into probabilty distribution.\n",
    "It exponaties them and normalizes. It outputs probability distribution - numbers that are positive and sum to one.\n",
    "\n",
    "![img](.//softmax_function.svg)\n",
    "\n",
    "For our example, it will allow to reference the output of the NN to the original approach with the statistical occurence counting."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We will make the neuron network return **log counts**\n",
    "# This is so that we can have a better intuition on what the NN returns. \n",
    "# Right now we can't have an intuition on the inner representation of neuron's weights.\n",
    "\n",
    "logits = xenc @ W # log-counts\n",
    "# This and a second line is a softmax!\n",
    "counts = logits.exp()\n",
    "prob = counts / counts.sum(1, keepdim=True) # WARN - if I don't do keepdim=True, this returns a wrong result! This is due to Pytorch broadcasting.\n",
    "print(f'prob.shape: {prob.shape}, prob first row: {prob[1]}, sum of the first row (should be one, those are probs): {prob[1].sum()}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# After the transformations above, we will interpret the output of each row of the resulting output matrix as distribution of probabilities.\n",
    "# In other words, neuron 0's output is a probability that the next character is start/stop, neuron 1's out is 'a', etc.\n",
    "\n",
    "def calculate_negative_log_likelihood_nn(xs, ys):\n",
    "    nlls = torch.zeros(len(xs))\n",
    "    for i in range(len(xs)):\n",
    "        x = xs[i].item()\n",
    "        y = ys[i].item()\n",
    "        p = prob[i, y] # for i-th example, how likely does the model think is the output from the training set?\n",
    "        nll = -p.log()\n",
    "        nlls[i] = nll\n",
    "        print(f'characters: {itos[x]},{itos[y]}, model ({y}-th neuron output) is {p}. Nll: {nll}')\n",
    "    return nlls\n",
    "\n",
    "nlls = calculate_negative_log_likelihood_nn(xs[:5],ys[:5])\n",
    "print(f'average negative log likelihood: {nlls.sum()/len(nlls)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Neural net training!\n",
    "We are doing the same thing as with micrograd. For the training set, we need to:\n",
    "1. Do forward pass - run the NN on all the inputs.\n",
    "2. Calculate the loss - for our case negative log likelihood \n",
    "3. Do the backward pass - calculate the gradients.\n",
    "4. Update the weigths (W matrix) with the new values of gradient."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "g = torch.Generator().manual_seed(2147483647)\n",
    "# Initialize 27 neurons, each with 27 inputs for one-hot encoded characters.\n",
    "W = torch.randn((27, 27), generator=g, requires_grad=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# xenc - our input for training data, one-hot encoded first characters of the bigrams\n",
    "# y - out output of the training data, for xenc[i] this is the expected next letter in the bigram.\n",
    "def train(iters=10):\n",
    "    for i in range(iters):\n",
    "        print(f'iteration {i}')\n",
    "        # Forward pass\n",
    "        logits = xenc @ W\n",
    "        counts = logits.exp()\n",
    "        softmax = counts/counts.sum(1, keepdim=True)\n",
    "        # Calculate loss - NLL\n",
    "        # The softmax is an output of NN for all inputs. So the formula below will return for i-th row the corresponding y[i]-th value.\n",
    "        # This column is the one we care about, as this is a training example. E.g. for 10th training example c->d so for 10th row we should \n",
    "        # take 4 column (d's index).\n",
    "        probs_for_examples = softmax[torch.arange(xenc.shape[0]), ys]\n",
    "        nll = -probs_for_examples.log()\n",
    "        loss = nll.mean()\n",
    "        print(f'loss: {loss}')\n",
    "        # Backward\n",
    "        W.grad = None # Reset the gradient\n",
    "        loss.backward()\n",
    "        # Update\n",
    "        W.data += -15.0*W.grad\n",
    "\n",
    "train(1024)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conculsion of the NN approach\n",
    "As seen above, the loss function (NLL) of the neural net approaches asymptotically the NLL of the \"counting\" approach (~2.45), so those two approaches end up in the same spot. Sweet!\n",
    "\n",
    "### Parallels between P matrix and W matrix\n",
    "The counting approach has P matrix with occurence counts/probabilites. This is equivalent to the output weights (e^weights more precisely, as those are log likelyhoods).\n",
    "Smoothing the P array was achieved by adding a constant C to each cell between normalizing. The same thing can be achieved for NN by incentivizing weights W to be near zero - e.g. by choosing a different initialization than normal distribution. This is because e^0 = 1, so normalized it's a uniform distribution for each \"row\" of W.\n",
    "\n",
    "You can incorporate this smoothiing into loss, by doing e.g.\n",
    "`loss = nll.mean() + 0.01*(W**2).mean()`\n",
    "This adds increases loss function for such composition of Ws that make them far from zero."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "g = torch.Generator().manual_seed(2147483647)\n",
    "# Sampling from the trained NN\n",
    "result = ''\n",
    "input = 0 # Start character\n",
    "while True:\n",
    "    ienc = F.one_hot(torch.tensor([input]), num_classes=27).float()\n",
    "    # Forward\n",
    "    logits = ienc @ W\n",
    "    # Softmax - transform the output of NN into distribution on probabilities\n",
    "    counts = logits.exp()\n",
    "    softmax = counts / counts.sum(1, keepdim=True)\n",
    "    input = torch.multinomial(softmax, num_samples=1, generator=g).item()\n",
    "    if input == 0: \n",
    "        break\n",
    "    else: \n",
    "        result += itos[input]\n",
    "\n",
    "print(result)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
